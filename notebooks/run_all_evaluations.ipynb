{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating All Models Using GPT-4 as an A/B Tester\n",
    "\n",
    "## Introduction\n",
    "This notebook contains code to evaluate all A/B tests across all prompts for a benchmark. The code reads in a list of unevaluated AB tests with prompts and responses from a database, runs parallel A/B tests with GPT-4 as a judge, and writes the results back to the database.\n",
    "\n",
    "## Setup\n",
    "Before running the code, make sure to install the necessary dependencies by setting up the Poetry environment. You will also need to set up a database connection by filling in the appropriate values in the cells below. You will also need to provide up an OpenAI API key and organization ID (if appropriate).\n",
    "\n",
    "Additionally, specify the benchmark and evaluating model to use for evaluation. The benchmark is the name of the benchmark to evaluate. The evaluating model is the name of the model to use as a judge for the A/B tests. The evaluating model must be a model that is already in the database."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY']=\"YOUR_API_KEY\"\n",
    "os.environ['OPENAI_ORGANIZATION']=\"YOUR_ORG_ID\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.environ['RDS_HOSTNAME']=\"YOUR_RDS_DB_HOSTNAME\"\n",
    "os.environ['RDS_DB_NAME']=\"YOUR_RDS_DB_NAME\"\n",
    "os.environ['RDS_USERNAME']=\"YOUR_RDS_USERNAME\"\n",
    "os.environ['RDS_PASSWORD']=\"YOUR_RDS_PASSWORD\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BENCHMARK = \"v1\"\n",
    "EVALUATING_MODEL = \"gpt-4-0613\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code Structure\n",
    "The code is split into several sections:\n",
    "\n",
    "### Imports\n",
    "The first section imports the necessary modules and functions.\n",
    "\n",
    "### Data Loading\n",
    "The second section loads the unevaluated AB test prompts from the database and converts them to requests for the OpenAI API.\n",
    "\n",
    "### Model Evaluation\n",
    "The third section evaluates the AB test prompts using the OpenAI cookbook.\n",
    "\n",
    "### Data Writing\n",
    "The fourth section parses the evaluation results and writes them back to the database.\n",
    "\n",
    "## Usage\n",
    "To use this code, simply run the notebook from start to finish. The results will be written back to the database automatically."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from db.db_utils import get_all_unevaluated_ab_test_prompts_for_benchmark, insert_evals_by_model_into_db\n",
    "from src.parse import db_game_to_dict, construct_request, parse_result_to_eval_entry, games_list_to_lookup\n",
    "from src.utils import read_jsonl, write_jsonl"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "incomplete_game_rows = await get_all_unevaluated_ab_test_prompts_for_benchmark(benchmark_name=BENCHMARK, eval_model_name=EVALUATING_MODEL)\n",
    "print(f\"{len(incomplete_game_rows)} incomplete games found\")\n",
    "if len(incomplete_game_rows) > 0:\n",
    "    print(f\"Example: {incomplete_game_rows[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "games = [db_game_to_dict(row) for row in incomplete_game_rows]\n",
    "print(f\"{len(games)} games parsed\")\n",
    "if len(games) > 0:\n",
    "    print(f\"Example: {games[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "requests = [construct_request(game) for game in games]\n",
    "write_jsonl(requests, 'requests.jsonl')\n",
    "print(f\"{len(requests)} requests written to file\")\n",
    "if len(requests) > 0:\n",
    "    print(f\"Example: {requests[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python ../src/api_request_parallel_processor.py \\\n",
    "  --requests_filepath requests.jsonl \\\n",
    "  --save_filepath requests_results.jsonl \\\n",
    "  --request_url https://api.openai.com/v1/chat/completions \\\n",
    "  --max_requests_per_minute 200 \\\n",
    "  --max_tokens_per_minute 40000 \\\n",
    "  --token_encoding_name cl100k_base \\\n",
    "  --max_attempts 5 \\\n",
    "  --logging_level 20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Writing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results, failed_lines = read_jsonl('requests_results.jsonl')\n",
    "\n",
    "if len(failed_lines):\n",
    "    print(f\"{len(failed_lines)} failed lines read from file\")\n",
    "    print(f\"Example: {failed_lines[0]}\")\n",
    "\n",
    "print(f\"{len(results)} results read from file\")\n",
    "if len(results) > 0:\n",
    "    print(f\"Example: {results[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "games_lookup = games_list_to_lookup(games)\n",
    "\n",
    "eval_entries = [parse_result_to_eval_entry(result, games_lookup) for result in results]\n",
    "print(f\"{len(eval_entries)} eval entries created\")\n",
    "if len(eval_entries) > 0:\n",
    "    print(f\"Example: {eval_entries[0]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "insert_evals_by_model_into_db(eval_entries)\n",
    "print(f\"{len(eval_entries)} eval entries inserted into db\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
